{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a91fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from math import ceil\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, Dropout, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b492593",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0642f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))\n",
    "\n",
    "# Save a word2vec dictionary.\n",
    "def save_word2vec(filename):\n",
    "    with open(os.path.join('/Users/haoxing/Documents/Work/Teaching/Machine learning for Finance/Codes/NLP/NLP_app', filename),'a' , encoding='utf-8') as f :\n",
    "        for k, v in word2vec.items():\n",
    "            line = k+' '+str(list(v)).strip('[]').replace(',','')+'\\n'\n",
    "            f.write(line)\n",
    "\n",
    "# Load a word2vec dictionary.\n",
    "def load_word2vec(filename):\n",
    "    word2vec = {}\n",
    "    with open(os.path.join('/Users/haoxing/Documents/Work/Teaching/Machine learning for Finance/Codes/NLP/NLP_app', filename), encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            try :\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vec = np.asarray(values[1:], dtype='float32')\n",
    "                word2vec[word] = vec\n",
    "            except :\n",
    "                None\n",
    "    return word2vec\n",
    "\n",
    "# read the repo in PATH and append the texts in a list\n",
    "def get_data(PATH):\n",
    "    list_dir = os.listdir(PATH)\n",
    "    texts = []\n",
    "    fund_names = []\n",
    "    out = display(progress(0, len(list_dir)-1), display_id=True)\n",
    "    for ii, filename in enumerate(list_dir) :\n",
    "        with open(PATH+'/'+filename, 'r', encoding=\"utf8\") as f :\n",
    "            txt = f.read()\n",
    "            try :\n",
    "                txt_split = txt.split('<head_breaker>')\n",
    "                summary = txt_split[1].strip()\n",
    "                fund_name = txt_split[0].strip()\n",
    "            except :\n",
    "                summary = txt\n",
    "                fund_name = ''\n",
    "        texts.append(summary)\n",
    "        fund_names.append(fund_name)\n",
    "        out.update(progress(ii, len(list_dir)-1))\n",
    "    return fund_names, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fe5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PATH = 'MutualFundSummary'\n",
    "SUMMARY_LABELS_PATH = 'MutualFundLabels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc54a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='544'\n",
       "            max='544',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            544\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fund_names, summaries = get_data(SUMMARY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758f9abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fund_name</th>\n",
       "      <th>Performance fee?</th>\n",
       "      <th>Ivestment Strategy</th>\n",
       "      <th>Leverage?</th>\n",
       "      <th>Portfolio composition</th>\n",
       "      <th>Concentration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000051931-18-000151</td>\n",
       "      <td>American Funds College 2018 Fund</td>\n",
       "      <td>None</td>\n",
       "      <td>Balanced Fund (Low Risk)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Investment grade securities</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000051931-18-000151</td>\n",
       "      <td>American Funds College 2021 Fund</td>\n",
       "      <td>None</td>\n",
       "      <td>Balanced Fund (Low Risk)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Investment grade securities</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000051931-18-000151</td>\n",
       "      <td>American Funds College 2024 Fund</td>\n",
       "      <td>None</td>\n",
       "      <td>Balanced Fund (Low Risk)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Investment grade securities</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000051931-18-000151</td>\n",
       "      <td>American Funds College 2027 Fund</td>\n",
       "      <td>None</td>\n",
       "      <td>Balanced Fund (Low Risk)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Investment grade securities</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000051931-18-000151</td>\n",
       "      <td>American Funds College 2030 Fund</td>\n",
       "      <td>None</td>\n",
       "      <td>Balanced Fund (Low Risk)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Investment grade securities</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                         fund_name Performance fee?  \\\n",
       "0  0000051931-18-000151  American Funds College 2018 Fund             None   \n",
       "1  0000051931-18-000151  American Funds College 2021 Fund             None   \n",
       "2  0000051931-18-000151  American Funds College 2024 Fund             None   \n",
       "3  0000051931-18-000151  American Funds College 2027 Fund             None   \n",
       "4  0000051931-18-000151  American Funds College 2030 Fund             None   \n",
       "\n",
       "         Ivestment Strategy Leverage?        Portfolio composition  \\\n",
       "0  Balanced Fund (Low Risk)       Yes  Investment grade securities   \n",
       "1  Balanced Fund (Low Risk)       Yes  Investment grade securities   \n",
       "2  Balanced Fund (Low Risk)       Yes  Investment grade securities   \n",
       "3  Balanced Fund (Low Risk)       Yes  Investment grade securities   \n",
       "4  Balanced Fund (Low Risk)       Yes  Investment grade securities   \n",
       "\n",
       "   Concentration  \n",
       "0    Diversified  \n",
       "1    Diversified  \n",
       "2    Diversified  \n",
       "3    Diversified  \n",
       "4    Diversified  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label = pd.read_csv(SUMMARY_LABELS_PATH)\n",
    "df_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61492879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset: the fund that did not have a label but is in the folder of summaries\n",
    "test_funds = [fund for fund in fund_names if fund not in list(df_label['fund_name'])]\n",
    "test_index = [fund_names.index(fund) for fund in fund_names if fund not in list(df_label['fund_name'])]\n",
    "\n",
    "test_summaries = [summary for summary in summaries if summaries.index(summary) in test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372a791a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_funds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37879eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5861c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Equity Long Only (Low Risk)          248\n",
       "Fixed Income Long Only (Low Risk)    130\n",
       "Balanced Fund (Low Risk)              84\n",
       "Long Short Funds (High Risk)           4\n",
       "Commodities Fund (Low Risk)            1\n",
       "Name: Ivestment Strategy, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label['Ivestment Strategy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f0ecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296                   Columbia Commodity Strategy Fund\n",
      "301               Columbia Mortgage Opportunities Fund\n",
      "423             Anchor Tactical Equity Strategies Fund\n",
      "424          Anchor Tactical Municipal Strategies Fund\n",
      "425    Dreyfus Alternative Diversifier Strategies Fund\n",
      "Name: fund_name, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[84, 85, 102, 112, 148]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The only four high-risk fund and only one commodities should be removed from our model for it has too small amounts\n",
    "remove_fund = df_label.loc[(df_label['Ivestment Strategy']=='Long Short Funds (High Risk)') | (df_label['Ivestment Strategy']=='Commodities Fund (Low Risk)'),'fund_name']\n",
    "print(remove_fund)\n",
    "remove_index = [fund_names.index(fund) for fund in fund_names if fund in list(remove_fund)]\n",
    "remove_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3de8a171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296    Columbia Commodity Strategy Fund\n",
      "Name: fund_name, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The second model:keep the four high-risk fund\n",
    "remove_fund1 = df_label.loc[df_label['Ivestment Strategy']=='Commodities Fund (Low Risk)','fund_name']\n",
    "print(remove_fund1)\n",
    "remove_index1 = [fund_names.index(fund) for fund in fund_names if fund in list(remove_fund1)]\n",
    "remove_index1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "905cbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_funds = [fund for fund in fund_names if fund not in list(remove_fund)]\n",
    "keep_fund_summaries = [summary for summary in summaries if summaries.index(summary) not in remove_index]\n",
    "keep_funds1 = [fund for fund in fund_names if fund not in list(remove_fund1)]\n",
    "keep_fund_summaries1 = [summary for summary in summaries if summaries.index(summary) not in remove_index1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff74804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540 540\n",
      "544 544\n"
     ]
    }
   ],
   "source": [
    "print(len(keep_funds),len(keep_fund_summaries))\n",
    "print(len(keep_funds1),len(keep_fund_summaries1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea29fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame(data={'fund_name':keep_funds, 'summary':keep_fund_summaries}) \n",
    "df_summary1 = pd.DataFrame(data={'fund_name':keep_funds1, 'summary':keep_fund_summaries1}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4276e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_summary.merge(df_label, on =['fund_name'], how ='left').dropna()\n",
    "df_merge1 = df_summary1.merge(df_label, on =['fund_name'], how ='left').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31fabba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Equity Long Only (Low Risk)          247\n",
       "Fixed Income Long Only (Low Risk)    130\n",
       "Balanced Fund (Low Risk)              84\n",
       "Name: Ivestment Strategy, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge['Ivestment Strategy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e33356d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Equity Long Only (Low Risk)          247\n",
       "Fixed Income Long Only (Low Risk)    130\n",
       "Balanced Fund (Low Risk)              84\n",
       "Long Short Funds (High Risk)           4\n",
       "Name: Ivestment Strategy, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge1['Ivestment Strategy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8f39cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fund_name</th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "      <th>Performance fee?</th>\n",
       "      <th>Ivestment Strategy</th>\n",
       "      <th>Leverage?</th>\n",
       "      <th>Portfolio composition</th>\n",
       "      <th>Concentration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>Variable Portfolio - Wells Fargo Short Duratio...</td>\n",
       "      <td>SUMMARY OF CTIVPSM – WELLS FARGO SHORT DURATIO...</td>\n",
       "      <td>0001193125-18-139001</td>\n",
       "      <td>None</td>\n",
       "      <td>Fixed Income Long Only (Low Risk)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Investment grade securities</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Western Asset Intermediate Maturity California...</td>\n",
       "      <td>Investment objective\\n\\nThe fund seeks to prov...</td>\n",
       "      <td>0001193125-18-091654</td>\n",
       "      <td>None</td>\n",
       "      <td>Fixed Income Long Only (Low Risk)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Investment grade securities</td>\n",
       "      <td>Concentrated by issuer / sector / jurisdiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Western Asset Intermediate Maturity New York M...</td>\n",
       "      <td>Investment objective\\n\\nThe fund seeks to prov...</td>\n",
       "      <td>0001193125-18-091654</td>\n",
       "      <td>None</td>\n",
       "      <td>Fixed Income Long Only (Low Risk)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Investment grade securities</td>\n",
       "      <td>Concentrated by issuer / sector / jurisdiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Western Asset Massachusetts Municipals Fund</td>\n",
       "      <td>Investment objective\\n\\nThe fund seeks to prov...</td>\n",
       "      <td>0001193125-18-091654</td>\n",
       "      <td>None</td>\n",
       "      <td>Fixed Income Long Only (Low Risk)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Investment grade securities</td>\n",
       "      <td>Concentrated by issuer / sector / jurisdiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>World Growth Fund</td>\n",
       "      <td>INVESTMENT OBJECTIVE\\nThe USAA World Growth Fu...</td>\n",
       "      <td>0001683863-18-000339</td>\n",
       "      <td>Some performance Fees</td>\n",
       "      <td>Equity Long Only (Low Risk)</td>\n",
       "      <td>No</td>\n",
       "      <td>Sub-investment grade securities or emerging ma...</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             fund_name  \\\n",
       "533  Variable Portfolio - Wells Fargo Short Duratio...   \n",
       "536  Western Asset Intermediate Maturity California...   \n",
       "537  Western Asset Intermediate Maturity New York M...   \n",
       "538        Western Asset Massachusetts Municipals Fund   \n",
       "539                                  World Growth Fund   \n",
       "\n",
       "                                               summary                    id  \\\n",
       "533  SUMMARY OF CTIVPSM – WELLS FARGO SHORT DURATIO...  0001193125-18-139001   \n",
       "536  Investment objective\\n\\nThe fund seeks to prov...  0001193125-18-091654   \n",
       "537  Investment objective\\n\\nThe fund seeks to prov...  0001193125-18-091654   \n",
       "538  Investment objective\\n\\nThe fund seeks to prov...  0001193125-18-091654   \n",
       "539  INVESTMENT OBJECTIVE\\nThe USAA World Growth Fu...  0001683863-18-000339   \n",
       "\n",
       "          Performance fee?                 Ivestment Strategy Leverage?  \\\n",
       "533                   None  Fixed Income Long Only (Low Risk)       Yes   \n",
       "536                   None  Fixed Income Long Only (Low Risk)       Yes   \n",
       "537                   None  Fixed Income Long Only (Low Risk)       Yes   \n",
       "538                   None  Fixed Income Long Only (Low Risk)       Yes   \n",
       "539  Some performance Fees        Equity Long Only (Low Risk)        No   \n",
       "\n",
       "                                 Portfolio composition  \\\n",
       "533                        Investment grade securities   \n",
       "536                        Investment grade securities   \n",
       "537                        Investment grade securities   \n",
       "538                        Investment grade securities   \n",
       "539  Sub-investment grade securities or emerging ma...   \n",
       "\n",
       "                                      Concentration  \n",
       "533                                     Diversified  \n",
       "536  Concentrated by issuer / sector / jurisdiction  \n",
       "537  Concentrated by issuer / sector / jurisdiction  \n",
       "538  Concentrated by issuer / sector / jurisdiction  \n",
       "539                                     Diversified  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7470437",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(df_merge['summary'], df_merge['Ivestment Strategy'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83e0918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_valid1, y_train1, y_valid1 = train_test_split(df_merge1['summary'], df_merge1['Ivestment Strategy'], test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f9148e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "896f7ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b9e5b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce5019f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words set\n",
    "stop_words = set(stopwords.words(\"english\")+list(string.punctuation)+['``',\"''\"]+[\"]\",\"[\",\"*\"]+['doe', 'ha', 'wa'])\n",
    "\n",
    "def tokenizer(txt):\n",
    "    \"\"\"tokenizer\n",
    "\n",
    "    Args:\n",
    "        txt (str): text to be tokenized\n",
    "\n",
    "    Returns:\n",
    "        filtered_sentence (list): list of tokenized string.  \n",
    "    \"\"\"\n",
    "    txt = txt.lower().replace('\\t', ' ').replace('\\n', ' ')\n",
    "    word_tokens = word_tokenize(txt)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = [w for w in filtered_sentence if re.sub(\"[^A-Za-z ]+\",'',w) != ''] \n",
    "    return filtered_sentence\n",
    "    \n",
    "train_text_words = np.concatenate([tokenizer(s) for s in X_train])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb86c4",
   "metadata": {},
   "source": [
    "## skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dee57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters for skip-gram model\n",
    "batch_size = 120\n",
    "num_epochs = 2\n",
    "# word2vec parameters\n",
    "embedding_size = 50 # size of embedding vector\n",
    "max_vocabulary_size = 5000 #  number of different words in vocabulary\n",
    "min_occurrence = 10 # words must appear at least 10 times\n",
    "skip_window = 3 # how many words to consider left and right\n",
    "num_skips = 4 # how many times to reuse an input to generate a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6d84342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dictionary and replace rare words with UNK token\n",
    "count = [('UNK', -1)]\n",
    "# Count the most common words\n",
    "count.extend(collections.Counter(train_text_words).most_common(max_vocabulary_size - 1))\n",
    "# Remove samples with less than 'min_occurrence' occurrences\n",
    "for i in range(len(count) - 1, -1, -1):\n",
    "    if count[i][1] < min_occurrence:\n",
    "        count.pop(i)\n",
    "    else:\n",
    "        # The collection is ordered, so stop when 'min_occurrence' is reached\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5667c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# give a unique id to each words in the vocabulary\n",
    "word2id = dict()\n",
    "for i, (word, _)in enumerate(count):\n",
    "    word2id[word] = i\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "vocab_size = len(id2word)\n",
    "\n",
    "unk_count = 0 # we still need to count the unkown words(the words that are not in the word2id)\n",
    "data = [] # turn words in train_text_words into id numbers and put them in data\n",
    "\n",
    "for word in train_text_words:\n",
    "    index = word2id.get(word, 0)\n",
    "    if index == 0: # if can't find the word in word2id\n",
    "        unk_count += 1\n",
    "    data.append(index) # append the id number\n",
    "count[0] = ('UNK', unk_count) # update the UNK value of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db441e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[data_point_index] = 1\n",
    "    return one_hot\n",
    "\n",
    "def batch_generator(batch_size, num_skips, skip_window, vocab_size):\n",
    "    \"\"\"function to generator batch for model training\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): size of batch\n",
    "        num_skips (int): how many times to reuse an input to generate a label\n",
    "        skip_window (int): how many words to consider left and right\n",
    "        vocab_size (int): size of the word2id/id2word\n",
    "    Yields:\n",
    "        batch_one_hot\n",
    "        labels_one_hot\n",
    "    \"\"\"    \n",
    "    data_idx = 0\n",
    "    while True:\n",
    "        assert num_skips <= 2 * skip_window\n",
    "        assert batch_size % num_skips == 0\n",
    "        labels = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        span = skip_window * 2 + 1\n",
    "        buffer = collections.deque(maxlen=span) #keep track of the visited indexes visited\n",
    "        if data_idx + span > len(data):\n",
    "            data_idx = 0\n",
    "            # stop the loop\n",
    "            break\n",
    "        buffer.extend(data[data_idx:data_idx + span])\n",
    "        data_idx += span\n",
    "        for i in range(batch_size // num_skips):  \n",
    "            # Take the context current word\n",
    "            context_words = [w for w in range(span) if w != skip_window]\n",
    "            # sample num_skips number of words\n",
    "            words_to_use = random.sample(context_words, num_skips)\n",
    "            for j, context_word in enumerate(words_to_use):\n",
    "                batch[i * num_skips + j] = buffer[skip_window]\n",
    "                labels[i * num_skips + j] = buffer[context_word]\n",
    "            if data_idx == len(data):\n",
    "                buffer.extend(data[0:span])\n",
    "                data_idx = span\n",
    "            else:\n",
    "                buffer.append(data[data_idx])\n",
    "                data_idx += 1\n",
    "        \n",
    "        data_idx = (data_idx + len(data) - span) % len(data)\n",
    "\n",
    "        # translate word index to on-hot\n",
    "        batch_one_hot = np.array([to_one_hot(b, vocab_size) for b in batch])\n",
    "        labels_one_hot = np.array([to_one_hot(l, vocab_size) for l in labels])\n",
    "        \n",
    "        yield batch_one_hot, labels_one_hot # output one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d15d192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the skip-gram model\n",
    "def word2vec_model():\n",
    "    input_w = Input(shape = (vocab_size,))\n",
    "    encoded = Dense(embedding_size, activation='linear')(input_w)\n",
    "    decoded = Dense(vocab_size, activation='softmax')(encoded)\n",
    "    autoencoder = Model(input_w, decoded)\n",
    "    encoder = Model(input_w, encoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return encoder, autoencoder\n",
    "\n",
    "encoder, autoencoder = word2vec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1eabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1416/4682 [========>.....................] - ETA: 22s - loss: 0.0571"
     ]
    }
   ],
   "source": [
    "autoencoder.fit(x = batch_generator(batch_size, num_skips, skip_window, vocab_size), steps_per_epoch=ceil(len(data) / batch_size), epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word2vec(filename):\n",
    "    with open(os.path.join('', filename),'a' , encoding='utf-8') as f :\n",
    "        for k, v in word2vec.items():\n",
    "            line = k+' '+str(list(v)).strip('[]').replace(',','')+'\\n'\n",
    "            f.write(line)\n",
    "\n",
    "# Load a word2vec dictionary.\n",
    "def load_word2vec(filename):\n",
    "    word2vec = {}\n",
    "    with open(os.path.join('', filename), encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            try :\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vec = np.asarray(values[1:], dtype='float32')\n",
    "                word2vec[word] = vec\n",
    "            except :\n",
    "                None\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49135a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the encoder to vectorize\n",
    "def vecotrize(word):\n",
    "    w_one_hot = to_one_hot(word2id[word], vocab_size)\n",
    "    return encoder.predict(np.array([w_one_hot]))[0]\n",
    "\n",
    "# create the word2vec dictionary then save it.\n",
    "word2vec = {w : vecotrize(w) for w in word2id.keys()}\n",
    "save_word2vec('train_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_tokenizer(text):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text.replace(\"'\",\" \"))]\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")+list(string.punctuation)+['``',\"''\",\"’\"]+[\"]\",\"[\",\"*\"]+['doe', 'ha', 'wa'] +['--']+ [''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 18\n",
    "tfidf = TfidfVectorizer(input='content', tokenizer=lemma_tokenizer, stop_words=stop_words, max_features=max_features)\n",
    "tfidf1 = TfidfVectorizer(input='content', tokenizer=lemma_tokenizer, stop_words=stop_words, max_features=max_features)\n",
    "# Fits the tfidf vecotizer on the train sample and create the training features.\n",
    "\n",
    "tfidf_train = tfidf.fit_transform(X_train) #update\n",
    "# Uses the vectorizer to create the test features.\n",
    "tfidf_train1 = tfidf1.fit_transform(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = tfidf.get_feature_names() # Common keywords\n",
    "print(key_words)\n",
    "key_words1 = tfidf1.get_feature_names() # Common keywords\n",
    "print(key_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b09b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_closer(word, n, word2vec):\n",
    "    vect = word2vec[word]\n",
    "    dist_dict = {k: cosine(v, vect) for k, v in word2vec.items()}\n",
    "    closer_words = []\n",
    "    for _ in range(n):\n",
    "        min_key = min(dist_dict.keys(), key=lambda k: dist_dict[k])\n",
    "        closer_words.append(min_key)\n",
    "        del dist_dict[min_key]\n",
    "    return closer_words\n",
    "\n",
    "##knowledge base\n",
    "def create_knowledge_base(num_neighbors, word2vec, key_words):\n",
    "    knowledge_base = set()\n",
    "    out = display(progress(0, len(key_words)-1), display_id=True)\n",
    "    for ii, key_word in enumerate(key_words) :\n",
    "        knowledge_base.add(key_word)\n",
    "        neighbors = []\n",
    "        try :\n",
    "            neighbors = get_n_closer(key_word, num_neighbors, word2vec)\n",
    "        except :\n",
    "            print(key_word + ' not in word2vec')\n",
    "\n",
    "        knowledge_base.update(neighbors)\n",
    "        \n",
    "        out.update(progress(ii, len(key_words)-1))\n",
    "    return knowledge_base\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base = create_knowledge_base(5, word2vec, key_words)\n",
    "knowledge_base1 = create_knowledge_base(5, word2vec, key_words1)\n",
    "print(knowledge_base)\n",
    "print(knowledge_base1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bfe25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a summary, the knowledge base and some hyper parameters and returns the \"num_sent\" sentences\n",
    "# of the summary that are closer to the the knowledge base in term of spacial distances.\n",
    "def extract_sentence_distance(summary, knowledge, n_closer, n_reject, num_sent):\n",
    "    # Split the summary into sentences.\n",
    "    sentences = sent_tokenize(summary)\n",
    "    sentence_scores = []\n",
    "    # Loop over the sentences.\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        # we tokenize and clean the sentence\n",
    "        tokens = tokenizer(sentence)\n",
    "\n",
    "        sentence_barycentre = np.zeros(embedding_size)\n",
    "        effective_len = 0\n",
    "        # Compute the barycentre of the sentence\n",
    "        for token in tokens :\n",
    "            try :\n",
    "                sentence_barycentre += np.array(word2vec[token])\n",
    "                effective_len += 1\n",
    "            except KeyError :\n",
    "                pass\n",
    "            except :\n",
    "                raise\n",
    "        \n",
    "        # Reject sentences with less than n_reject words in our word2vec map\n",
    "        if effective_len <= n_reject :\n",
    "            sentence_scores.append(1)    \n",
    "\n",
    "        else :\n",
    "            sentence_barycentre = sentence_barycentre/effective_len\n",
    "            # Compute the distance sentece_barycentre -> words in our knowledge base\n",
    "            barycentre_distance = [cosine(sentence_barycentre, word2vec[key_word]) for key_word in knowledge]\n",
    "            barycentre_distance.sort()\n",
    "            # Create the score of the sentence by averaging the \"n_closer\" smallest distances\n",
    "            score = np.mean(barycentre_distance[:n_closer])\n",
    "            sentence_scores.append(score)\n",
    "    # Select the \"num_sent\" sentences that have the smallest score (smallest distance score with the knowledge base)\n",
    "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
    "    top_sentences = sentences[:num_sent]\n",
    "    return ' '.join(top_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462614a0",
   "metadata": {},
   "source": [
    "## Q4 Measure Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d29c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the train,validation and test dataframe\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_valid_df = pd.DataFrame(X_valid)\n",
    "X_train_df1 = pd.DataFrame(X_train1)\n",
    "X_valid_df1 = pd.DataFrame(X_valid1)\n",
    "X_test_df = pd.DataFrame({'summary':test_summaries})\n",
    "X_test_df1 = pd.DataFrame({'summary':test_summaries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4de0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df['sentences_distance'] = X_train_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
    "X_valid_df['sentences_distance'] = X_valid_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
    "X_train_df1['sentences_distance'] = X_train_df1.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base1, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
    "X_valid_df1['sentences_distance'] = X_valid_df1.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base1, n_closer=10, n_reject=5, num_sent=5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ef17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df['sentences_distance'] = X_test_df.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base, n_closer=10, n_reject=5, num_sent=5), axis=1)\n",
    "X_test_df1['sentences_distance'] = X_test_df1.apply(lambda x : extract_sentence_distance(x['summary'], knowledge_base1, n_closer=10, n_reject=5, num_sent=5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6de464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_match(summary, knowledge, num_sent):\n",
    "    sentences = sent_tokenize(summary)\n",
    "    sentence_scores = []\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        set_tokens = set(tokenizer(sentence))\n",
    "\n",
    "        # Find the number of common words between the knowledge base and the sentence\n",
    "        inter_knwoledge = set_tokens.intersection(knowledge)\n",
    "\n",
    "        sentence_scores.append(len(inter_knwoledge))\n",
    "\n",
    "    sentence_scores, sentences = zip(*sorted(zip(sentence_scores, sentences)))\n",
    "    top_sentences = sentences[len(sentences)-num_sent-1:]\n",
    "    return ' '.join(top_sentences)\n",
    "X_train_df['sentences_match'] = X_train_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
    "X_valid_df['sentences_match'] = X_valid_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
    "X_train_df1['sentences_match'] = X_train_df1.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base1, num_sent=5), axis=1)\n",
    "X_valid_df1['sentences_match'] = X_valid_df1.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base1, num_sent=5), axis=1)\n",
    "X_test_df['sentences_match'] = X_test_df.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base, num_sent=5), axis=1)\n",
    "X_test_df1['sentences_match'] = X_test_df1.apply(lambda x : extract_sentence_match(x['summary'], knowledge_base1, num_sent=5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce train_X and test_X\n",
    "train_X = X_train_df['sentences_match'].values\n",
    "train_X = [' '.join(tokenizer(txt)) for txt in train_X]\n",
    "\n",
    "valid_X = X_valid_df['sentences_match'].values\n",
    "valid_X = [' '.join(tokenizer(txt)) for txt in valid_X]\n",
    "\n",
    "test_X = X_test_df['sentences_match'].values\n",
    "test_X = [' '.join(tokenizer(txt)) for txt in test_X]\n",
    "\n",
    "train_X1 = X_train_df1['sentences_match'].values\n",
    "train_X1 = [' '.join(tokenizer(txt)) for txt in train_X1]\n",
    "\n",
    "valid_X1 = X_valid_df1['sentences_match'].values\n",
    "valid_X1 = [' '.join(tokenizer(txt)) for txt in valid_X1]\n",
    "\n",
    "test_X1 = X_test_df1['sentences_match'].values\n",
    "test_X1 = [' '.join(tokenizer(txt)) for txt in test_X1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce train_y and valid_y\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoded_train_y = encoder.fit_transform(y_train)\n",
    "label_train_y = np_utils.to_categorical(encoded_train_y,num_classes=3)\n",
    "\n",
    "\n",
    "encoded_valid_y = encoder.transform(y_valid)\n",
    "label_valid_y = np_utils.to_categorical(encoded_valid_y,num_classes=3)\n",
    "\n",
    "encoded_train_y1 = encoder.fit_transform(y_train1)\n",
    "label_train_y1 = np_utils.to_categorical(encoded_train_y1,num_classes=4)\n",
    "\n",
    "\n",
    "encoded_valid_y1 = encoder.transform(y_valid1)\n",
    "label_valid_y1 = np_utils.to_categorical(encoded_valid_y1,num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ae791",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31068eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 2500 # Size of the vocabulary used. we only consider the 2500 most common words. The other words are removed from the texts.\n",
    "maxlen = 150 # Number of word considered for each document. we cut or lengthen the texts to have texts of 150 words.\n",
    "word_dimension = 50 # dimension of our word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_tokenizer = Tokenizer(num_words=num_words)\n",
    "keras_tokenizer1 = Tokenizer(num_words=num_words)\n",
    "keras_tokenizer.fit_on_texts(train_X)\n",
    "keras_tokenizer1.fit_on_texts(train_X1)\n",
    "word_index = keras_tokenizer.word_index\n",
    "word_index1 = keras_tokenizer1.word_index\n",
    "sequences_train = keras_tokenizer.texts_to_sequences(train_X)\n",
    "sequences_valid = keras_tokenizer.texts_to_sequences(valid_X)\n",
    "sequences_test = keras_tokenizer.texts_to_sequences(test_X)\n",
    "sequences_train1 = keras_tokenizer1.texts_to_sequences(train_X1)\n",
    "sequences_valid1 = keras_tokenizer1.texts_to_sequences(valid_X1)\n",
    "sequences_test1 = keras_tokenizer1.texts_to_sequences(test_X1)\n",
    "# truncate or lenthen each text so they have the same length.\n",
    "feature_train = pad_sequences(sequences_train, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
    "feature_valid = pad_sequences(sequences_valid, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
    "feature_test = pad_sequences(sequences_test, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
    "feature_train1 = pad_sequences(sequences_train1, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
    "feature_valid1 = pad_sequences(sequences_valid1, maxlen=maxlen, dtype=float, padding='post', truncating='post')\n",
    "feature_test1 = pad_sequences(sequences_test1, maxlen=maxlen, dtype=float, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, word_dimension))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word2vec.get(word)   \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix1 = np.zeros((len(word_index1) + 1, word_dimension))\n",
    "for word, i in word_index1.items():\n",
    "    embedding_vector = word2vec.get(word)   \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix1[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055dd946",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2da57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_train_y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe9b7de",
   "metadata": {},
   "source": [
    "## Q5 training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5c5d0",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65420cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN_model():\n",
    "    CNN = Sequential()\n",
    "    # The Embedding layer takes the embedding matrix as an argument and transform the inputed the sequences of index to sequences of vectors.\n",
    "    CNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False))\n",
    "\n",
    "\n",
    "    CNN.add(Convolution1D(64, 5, activation = 'relu'))\n",
    "    CNN.add(MaxPooling1D(pool_size = 5))\n",
    "\n",
    "    CNN.add(Convolution1D(32, 5, activation = 'relu'))\n",
    "    CNN.add(MaxPooling1D(pool_size = 5))\n",
    "\n",
    "    CNN.add(Flatten())\n",
    "    CNN.add(Dense(units = 128 , activation = 'relu'))\n",
    "    CNN.add(Dropout(0.5))\n",
    "    CNN.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "    CNN.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return CNN\n",
    "    \n",
    "CNN_model = create_CNN_model()\n",
    "print(feature_train)\n",
    "CNN_history = CNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da97634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN_model1():\n",
    "    CNN = Sequential()\n",
    "    # The Embedding layer takes the embedding matrix as an argument and transform the inputed the sequences of index to sequences of vectors.\n",
    "    CNN.add(Embedding(len(word_index1) + 1, word_dimension, weights=[embedding_matrix1], input_length = maxlen, trainable=False))\n",
    "\n",
    "\n",
    "    CNN.add(Convolution1D(64, 5, activation = 'relu'))\n",
    "    CNN.add(MaxPooling1D(pool_size = 5))\n",
    "\n",
    "    CNN.add(Convolution1D(32, 5, activation = 'relu'))\n",
    "    CNN.add(MaxPooling1D(pool_size = 5))\n",
    "\n",
    "    CNN.add(Flatten())\n",
    "    CNN.add(Dense(units = 128 , activation = 'relu'))\n",
    "    CNN.add(Dropout(0.5))\n",
    "    CNN.add(Dense(units = 4, activation = 'softmax'))\n",
    "\n",
    "    CNN.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return CNN\n",
    "    \n",
    "CNN_model1 = create_CNN_model1()\n",
    "print(feature_train)\n",
    "CNN_history1 = CNN_model1.fit(feature_train1, label_train_y1, epochs=800, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a2509",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_CNN = CNN_model.predict(feature_valid)\n",
    "y_valid_CNN1 = CNN_model1.predict(feature_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_valid_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the validation vector \n",
    "valid_y_CNN = y_valid_CNN.copy()\n",
    "for i in range(len(y_valid_CNN)):\n",
    "    j = np.where(y_valid_CNN[i] == np.amax(y_valid_CNN[i]))\n",
    "    valid_y_CNN[i] = [0, 0, 0]\n",
    "    valid_y_CNN[i][j] = 1\n",
    "valid_y_CNN1 = y_valid_CNN1.copy()\n",
    "for i in range(len(y_valid_CNN1)):\n",
    "    j = np.where(y_valid_CNN1[i] == np.amax(y_valid_CNN1[i]))\n",
    "    valid_y_CNN1[i] = [0, 0, 0, 0]\n",
    "    valid_y_CNN1[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea989d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(CNN_history.history['accuracy'])\n",
    "plt.title('CNN Model accuracy with class=3')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4edd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss values\n",
    "plt.plot(CNN_history.history['loss'])\n",
    "plt.title('CNN Model loss with class=3')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(CNN_history1.history['accuracy'])\n",
    "plt.title('CNN Model accuracy with class=4')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89271ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss values\n",
    "plt.plot(CNN_history1.history['loss'])\n",
    "plt.title('CNN Model loss with class=4')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20070774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(label_valid_y,valid_y_CNN))\n",
    "\n",
    "print(classification_report(label_valid_y,valid_y_CNN))\n",
    "\n",
    "print(\"auc score: \",roc_auc_score(label_valid_y,valid_y_CNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84763ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(label_valid_y1,valid_y_CNN1))\n",
    "\n",
    "print(classification_report(label_valid_y1,valid_y_CNN1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb906f3a",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf0bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model():\n",
    "    RNN = Sequential()\n",
    "    RNN.add(Embedding(len(word_index) + 1, word_dimension, weights=[embedding_matrix], input_length = maxlen, trainable=False))\n",
    "\n",
    "    RNN.add(Bidirectional(LSTM(word_dimension)))\n",
    "    RNN.add(Dense(word_dimension, activation='relu'))\n",
    "    RNN.add(Dense(3, activation='softmax'))\n",
    "    RNN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "    return RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = create_RNN_model()\n",
    "RNN_history = RNN_model.fit(feature_train, label_train_y, epochs=800, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddece9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy\n",
    "plt.plot(RNN_history.history['accuracy'])\n",
    "plt.title(' RNN Model accuracy with class=3')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss values\n",
    "plt.plot(RNN_history.history['loss'])\n",
    "plt.title('RNN Model loss with class=3')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a742e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation with the RNN \n",
    "y_valid_RNN = RNN_model.predict(feature_valid)\n",
    "valid_y_RNN = y_valid_RNN.copy()\n",
    "for i in range(len(y_valid_RNN)):\n",
    "    j = np.where(y_valid_RNN[i] == np.amax(y_valid_RNN[i]))\n",
    "    valid_y_RNN[i] = [0, 0, 0]\n",
    "    valid_y_RNN[i][j] = 1\n",
    "  \n",
    "# print acc and report\n",
    "print(accuracy_score(label_valid_y,valid_y_RNN))\n",
    "\n",
    "print(classification_report(label_valid_y,valid_y_RNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model1():\n",
    "    RNN = Sequential()\n",
    "    RNN.add(Embedding(len(word_index1) + 1, word_dimension, weights=[embedding_matrix1], input_length = maxlen, trainable=False))\n",
    "\n",
    "    RNN.add(Bidirectional(LSTM(word_dimension)))\n",
    "    RNN.add(Dense(word_dimension, activation='relu'))\n",
    "    RNN.add(Dense(4, activation='softmax'))\n",
    "    RNN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "    return RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee6dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model1 = create_RNN_model1()\n",
    "RNN_history1 = RNN_model1.fit(feature_train1, label_train_y1, epochs=800, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy\n",
    "plt.plot(RNN_history1.history['accuracy'])\n",
    "plt.title(' RNN Model accuracy with class=4')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss values\n",
    "plt.plot(RNN_history1.history['loss'])\n",
    "plt.title('RNN Model loss with class=4')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb97222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation with the RNN \n",
    "y_valid_RNN1 = RNN_model1.predict(feature_valid1)\n",
    "valid_y_RNN1 = y_valid_RNN1.copy()\n",
    "for i in range(len(y_valid_RNN1)):\n",
    "    j = np.where(y_valid_RNN1[i] == np.amax(y_valid_RNN1[i]))\n",
    "    valid_y_RNN1[i] = [0, 0, 0, 0]\n",
    "    valid_y_RNN1[i][j] = 1\n",
    "  \n",
    "# print acc and report\n",
    "print(accuracy_score(label_valid_y1,valid_y_RNN1))\n",
    "\n",
    "print(classification_report(label_valid_y1,valid_y_RNN1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f1faf",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0df6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_CNN = CNN_model.predict(feature_test)\n",
    "pred_y_CNN = y_pred_CNN.copy()\n",
    "for i in range(len(y_pred_CNN)):\n",
    "    j = np.where(y_pred_CNN[i] == np.amax(y_pred_CNN[i]))\n",
    "    pred_y_CNN[i] = [0, 0, 0]\n",
    "    pred_y_CNN[i][j] = 1\n",
    "    \n",
    "y_pred_RNN = RNN_model.predict(feature_test)\n",
    "pred_y_RNN = y_pred_RNN.copy()\n",
    "for i in range(len(y_pred_RNN)):\n",
    "    j = np.where(y_pred_RNN[i] == np.amax(y_pred_RNN[i]))\n",
    "    pred_y_RNN[i] = [0, 0, 0]\n",
    "    pred_y_RNN[i][j] = 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e16a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred_y = pd.Series(np.zeros(len(pred_y_CNN)))\n",
    "for i in range(len(pred_y_CNN)):\n",
    "    j = np.where(pred_y_CNN[i] == 1)\n",
    "    if j == (np.array([0]),):\n",
    "        cnn_pred_y[i] = 'Balanced Fund (Low Risk)'\n",
    "    elif j == (np.array([1]),):\n",
    "        cnn_pred_y[i] = 'Equity Long Only (Low Risk)'\n",
    "    elif j == (np.array([2]),):\n",
    "        cnn_pred_y[i] = 'Fixed Income Long Only (Low Risk)'\n",
    "\n",
    "\n",
    "cnn_predict = pd.DataFrame({'fund name':test_funds,'CNN prediction':cnn_pred_y})\n",
    "print(cnn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_pred_y = pd.Series(np.zeros(len(pred_y_RNN)))\n",
    "for i in range(len(pred_y_RNN)):\n",
    "    j = np.where(pred_y_RNN[i] == 1)\n",
    "    if j == (np.array([0]),):\n",
    "        rnn_pred_y[i] = 'Balanced Fund (Low Risk)'\n",
    "    elif j == (np.array([1]),):\n",
    "        rnn_pred_y[i] = 'Equity Long Only (Low Risk)'\n",
    "    elif j == (np.array([2]),):\n",
    "        rnn_pred_y[i] = 'Fixed Income Long Only (Low Risk)'\n",
    "\n",
    "\n",
    "rnn_predict = pd.DataFrame({'fund name':test_funds,'RNN prediction':rnn_pred_y})\n",
    "print(rnn_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5b4f12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638fffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
